{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('ign.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df[['title','score_phrase','platform','score','genre','editors_choice','release_year','release_month','release_day']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the editor's choice 'Y'/'N' to 1/0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.editors_choice = df['editors_choice'].apply(lambda x: 1 if x=='Y' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating a list of platforms whose data points in our dataset is less than 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sparse_platform = [k for k,v in dict(df.platform.value_counts()).items() if v < 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random sampling on df to get the training and testing data. Split ratio 70:30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing all the sparse platform entries from the test and appending it in train. This is done to avoid key error which might have occured had the test set included all the data points for a sparse platform. Moreover, the data points for these sparse platforms are so less that the predicted target wouldn't have been accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "append_in_train = test[test.platform.isin(sparse_platform)] \n",
    "append_in_train2 = test[test.release_year==1970] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = test[~test.platform.isin(sparse_platform)] \n",
    "test = test[test.release_year!=1970] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = train.append(append_in_train, ignore_index=True)\n",
    "train = train.append(append_in_train2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainlabels = train.editors_choice\n",
    "testlabels = test.editors_choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm taking both score and score_phrase (even if they are the same thing!) because of the amount of information they provide for determining the target variable is huge (see Visualization notebook for details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = train[['score_phrase','platform','score','release_year','release_month','release_day']]\n",
    "test = test[['score_phrase','platform','score','release_year','release_month','release_day']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing some label encoders to encode categorical string('score phrase', 'platform' and 'release year') into categorical integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sple = LabelEncoder()\n",
    "ple = LabelEncoder()\n",
    "yle = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train[['score_phrase']] = sple.fit_transform(train[['score_phrase']])\n",
    "train[['platform']] = ple.fit_transform(train[['platform']])\n",
    "train[['release_year']] = yle.fit_transform(train[['release_year']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test[['score_phrase']] = sple.transform(test[['score_phrase']])\n",
    "test[['platform']] = ple.transform(test[['platform']])\n",
    "test[['release_year']] = yle.transform(test[['release_year']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large Margin Classifiers seems like a good choice since we have attributes like score phrase and score contributing so much information (see Visualizations notebook). It seems like our data is well seperated, and hence large margin classifiers.\n",
    "Also, I've kept the regularization term a little less that the default 1 (increasing the regularization), to avoid the model to overfit, due to large seperation between score phrases and score attributes and also because score phrase and score are essentially the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LinearSVC(C = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(train,trainlabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, a leaderboard of scores could be helpful for me to improve on this basic solution, but anyway 92% on a 1:5 skewed binary class prediction seems just fair, if not so impressive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.score(test,testlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Coefficients for the learned model.\n",
    "model.coef_"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
